{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e10f375a",
   "metadata": {},
   "source": [
    "# Telecom Churn Case Study\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cc73c5",
   "metadata": {},
   "source": [
    "# Problem Statement\n",
    "\n",
    "## Business problem overview\n",
    "\n",
    ". In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.\n",
    "\n",
    ". For many incumbent operators, retaining high profitable customers is the number one business goal.\n",
    "\n",
    ". To reduce customer churn, telecom companies need to predict which customers are at high risk of churn.\n",
    "\n",
    ". In this project, we will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a74f34",
   "metadata": {},
   "source": [
    "# Definitions of churn\n",
    ". There are various ways to define churn, such as:\n",
    "\n",
    "## Revenue-based churn:\n",
    ". Customers who have not utilised any revenue-generating facilities such as mobile internet, outgoing calls, SMS etc. over a given period of time. One could also use aggregate metrics such as ‘customers who have generated less than INR 4 per month in total/average/median revenue’.\n",
    "\n",
    "The main shortcoming of this definition is that there are customers who only receive calls/SMSes from their wage-earning counterparts, i.e. they don’t generate revenue but use the services. For example, many users in rural areas only receive calls from their wage-earning siblings in urban areas.\n",
    "\n",
    "## Usage-based churn:\n",
    "Customers who have not done any usage, either incoming or outgoing - in terms of calls, internet etc. over a period of time.\n",
    "\n",
    "A potential shortcoming of this definition is that when the customer has stopped using the services for a while, it may be too late to take any corrective actions to retain them. For e.g., if you define churn based on a ‘two-months zero usage’ period, predicting churn could be useless since by that time the customer would have already switched to another operator.\n",
    "\n",
    "In this project, we will use the usage-based definition to define churn."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7be816d",
   "metadata": {},
   "source": [
    "## Objective\n",
    ". To Predict the customers who are about to churn from a telecom operator . Business Objective is to predict the High Value Customers only . We need to predict Churn on the basis of Action Period (Churn period data needs to be deleted after labelling) Churn would be based on Usage\n",
    "\n",
    "## Requirement:\n",
    ". Churn Prediction Model . Best Predictor Variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a916e34c",
   "metadata": {},
   "source": [
    "## Steps to Approach The Best Solution For This Case Study\n",
    "There are mainly 6 steps\n",
    "\n",
    "#### Step 1 :\n",
    ". Data reading . Data Understanding . Data Cleaning Imputing missing values\n",
    "\n",
    "#### Step-2 :\n",
    "Need to Filter high value customers\n",
    "\n",
    "#### Step-3 :\n",
    "Derive churn need to Derive the Target Variable\n",
    "\n",
    "#### Step-4 :\n",
    "Data Preparation .Derived variable .EDA  \n",
    "\n",
    "#### Step-5 :\n",
    "Data modeling\n",
    ".Split data in to train and test sets .Performing Scaling . Handle class imbalance . Dimensionality Reduction using PCA .Classification models to predict Churn (Use various Models )\n",
    "\n",
    "#### Step-6 :\n",
    ".Model Evaluation .Prepare Model for Predictor variables selection (Prepare multiple models & choose the best one)\n",
    "\n",
    "Finally we need to give best Summarize to the company\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44545443",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38f25460",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1abddeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To suppress the warnings which will be raised\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Displaying all Columns without restrictions\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773c83fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from imblearn.metrics import sensitivity_specificity_support\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "from sklearn import metrics\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.decomposition import IncrementalPCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657549c4",
   "metadata": {},
   "source": [
    "# Step 1 :\n",
    "\n",
    "## i.Data reading\n",
    "\n",
    "## ii.Data Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d55fe9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df = pd.read_csv(\"telecom_churn_data (1).csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f3f886",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Looking at data statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccad92a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "472291f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature type summary\n",
    "df.info(verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dd6b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for null values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb44ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the null value percentage\n",
    "df.isna().sum()/df.isna().count()*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e353c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the duplicates\n",
    "df.drop_duplicates(subset=None, inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adef49e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the size of data\n",
    "df.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73d131a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the axes of data\n",
    "df.axes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523d17f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the dimensions of data\n",
    "df.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d373938",
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of columns\n",
    "pd.DataFrame(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4bb0a9",
   "metadata": {},
   "source": [
    "## iii. Data Cleaning , Imputing missing values¶"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b37852",
   "metadata": {},
   "source": [
    "Checking the column values where users didn't use the services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f514781",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segregating the columns related to recharge\n",
    "rech=[]\n",
    "for i in df.columns:\n",
    "    if 'rech' in i:\n",
    "        rech.append(i)\n",
    "rech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa5a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the null values in these columns for each month specifically\n",
    "\n",
    "for j in [6,7,8,9]:\n",
    "    print(j,'th month ****************************************')\n",
    "    for k in rech:\n",
    "        if str(j) in k:\n",
    "            print(k,'-> ',df[k].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cac8f19f",
   "metadata": {},
   "source": [
    "As the number of null values present in these columns are equal , hence the user hasn't use the service at all.\n",
    "Thus we can impute it with Zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2855bac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking the columns for imputing with 0\n",
    "zero_impute=[]\n",
    "for i in ['date_of_last_rech_data_','total_rech_data_','max_rech_data_','count_rech_2g_','count_rech_3g_','av_rech_amt_data_']:\n",
    "        for j in [6,7,8,9]:\n",
    "            zero_impute.append(i+str(j))\n",
    "zero_impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc874cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imputing the columns with zero\n",
    "for i in zero_impute:\n",
    "    df[i].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74ce9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking after imputation\n",
    "df[zero_impute].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b2a637",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the percentage of null values in each column\n",
    "100*(df.isna().sum()/df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cada7797",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segregating the columns which has more than 70 % null values\n",
    "col_to_drop=[]\n",
    "for i in df.columns:\n",
    "    if 100*(df[i].isna().sum()/df.shape[0])>70:\n",
    "        col_to_drop.append(i)\n",
    "col_to_drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "381b9799",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the high missing value columns\n",
    "df.drop(col_to_drop,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9610b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segregating the date columns\n",
    "date_col=[]\n",
    "for i in df.columns:\n",
    "    if 'date' in i:\n",
    "        date_col.append(i)\n",
    "date_col    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ab2174",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the date columns as we have already utilised the information present in date columns for imputation in other columns\n",
    "df.drop(date_col,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cedbaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the shape \n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e7c38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the null value percentage again\n",
    "null_per=100*(df.isna().sum()/df.shape[0])\n",
    "null_per[null_per>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1db2950",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segregating the cols\n",
    "for i in null_per[null_per>0].index.to_list():\n",
    "    df[i].fillna(df[i].median(),inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef56e27c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the null value percentages to verify\n",
    "100*(df.isna().sum()/df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0973b199",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deleting the id columns as it's not required\n",
    "df.drop(['mobile_number', 'circle_id'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88316964",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the unique values in each column\n",
    "df.nunique().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7321e8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segregating the columns which has 1 unique value\n",
    "value_1=[]\n",
    "for i in df.columns:\n",
    "    if df[i].nunique()==1:\n",
    "        value_1.append(i)\n",
    "value_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbf72a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping the columns which has 1 unique values\n",
    "df.drop(value_1,1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d406b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the unique values in each column again\n",
    "df.nunique().sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa14edcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the shape\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5a2453",
   "metadata": {},
   "source": [
    "# Step 2:\n",
    "\n",
    "## Filter high-value customers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c82ff8ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the total data recharge amount in months 6 and 7\n",
    "df['total_rech_amt_data_6']=df['av_rech_amt_data_6'] * df['total_rech_data_6']\n",
    "df['total_rech_amt_data_7']=df['av_rech_amt_data_7'] * df['total_rech_data_7']\n",
    "df['total_rech_amt_data_8']=df['av_rech_amt_data_8'] * df['total_rech_data_8']\n",
    "\n",
    "# Calculating the total recharge amount in months 6 and 7\n",
    "df['overall_rech_amt_6'] = df['total_rech_amt_data_6'] + df['total_rech_amt_6']\n",
    "df['overall_rech_amt_7'] = df['total_rech_amt_data_7'] + df['total_rech_amt_7']\n",
    "df['overall_rech_amt_8'] = df['total_rech_amt_data_8'] + df['total_rech_amt_8']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e966348d",
   "metadata": {},
   "source": [
    "As we derived the new features for the good phase, we drop the original features to avoid redundancy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b8eb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the corresponding original features\n",
    "df.drop(['total_rech_amt_data_6','av_rech_amt_data_6', 'total_rech_data_6','total_rech_amt_6',\n",
    "                  'total_rech_amt_data_7','av_rech_amt_data_7', 'total_rech_data_7','total_rech_amt_7',\n",
    "                  'total_rech_amt_data_8','av_rech_amt_data_8', 'total_rech_data_8','total_rech_amt_8'],\n",
    "                  axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c41328e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating the average recharge done by customer in months June and July(i.e. 6th and 7th month)\n",
    "df['avg_rech_amt_6_7'] = (df['overall_rech_amt_6'] + df['overall_rech_amt_7'])/2\n",
    "\n",
    "# Finding the value of 70th percentage to find the HVC\n",
    "HVC_criteria = df['avg_rech_amt_6_7'].quantile(0.7)\n",
    "print(\"\\nThe 70th quantile value to determine the High Value Customer is: \",HVC_criteria,\"\\n\")\n",
    "\n",
    "# Filtering the data to the top 30% considered as High Value Customer\n",
    "HVC_data = df[df['avg_rech_amt_6_7'] >= HVC_criteria]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eefd458",
   "metadata": {},
   "source": [
    "The 70th quantile value to determine the High Value Customer is:  478.0 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992569e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the shape\n",
    "HVC_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6805f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some of the columns were spelt wrongly for 'vbc_3g_*', lets correct them.\n",
    "HVC_data.rename(columns = {'aug_vbc_3g':'vbc_3g_8', 'jun_vbc_3g': 'vbc_3g_6', 'jul_vbc_3g' : 'vbc_3g_7', 'sep_vbc_3g' : 'vbc_3g_9'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b820b68",
   "metadata": {},
   "source": [
    "# Step 3:\n",
    "\n",
    "# Derive churn\n",
    ". Deriving Churn feature based on Usage based Churn\n",
    ". Derive churn means hear we are using 9 month(The ‘churn’ phase) data , To get the target variable(In this case stydy they did not provide any target variable we have to derive it from churn phase data) For that, we need to find the derive churn variable using total_ic_mou_9,total_og_mou_9,vol_2g_mb_9 and vol_3g_mb_9 attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1756cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a list of all the usage columns\n",
    "churn_col=['total_ic_mou_9','total_og_mou_9','vol_2g_mb_9','vol_3g_mb_9']\n",
    "\n",
    "# Creating the Churn column by adding all the churn variables\n",
    "HVC_data[\"Churn\"] = np.where(HVC_data[churn_col].sum(axis=1) == 0, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09367d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Churn feature\n",
    "HVC_data[\"Churn\"].value_counts(normalize=True) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa13a7",
   "metadata": {},
   "source": [
    "Removing all the variables related to churn phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f88ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating list of all columns related to month_9\n",
    "nine_month_cols=[]\n",
    "\n",
    "for i in HVC_data.columns:\n",
    "    if \"_9\" in i:\n",
    "        nine_month_cols.append(i)\n",
    "        \n",
    "nine_month_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762acf57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping Month 9 columns\n",
    "\n",
    "HVC_data=HVC_data.drop(nine_month_cols, axis=1)\n",
    "HVC_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5d618d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-Checking the columns info\n",
    "HVC_data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df62ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-Checking the columns info\n",
    "HVC_data.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02aa3d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-Checking missing Data\n",
    "missing_data_HVC = pd.DataFrame({\"total_missing\":HVC_data.isnull().sum() , \"perc_missing\":HVC_data.isnull().sum()/HVC_data.shape[0]*100})\n",
    "missing_data_HVC[missing_data_HVC[\"perc_missing\"]>0].sort_values(by=\"perc_missing\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c4bdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finding the unique values for all the columns in high value customers data set\n",
    "#uniqueValues = pd.DataFrame(columns=[\"Value\", \"UniqueValueCount\"])\n",
    "for col in HVC_data.columns:\n",
    "    values = HVC_data[col].value_counts()\n",
    "    \n",
    "    print(values)\n",
    "    print(\"--------------------------------------------\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a169ed68",
   "metadata": {},
   "outputs": [],
   "source": [
    "HVC_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632111ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(HVC_data.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8426cd",
   "metadata": {},
   "source": [
    "## finding churn and non churn percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c48689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets find out churn/non churn percentage\n",
    "print((HVC_data['Churn'].value_counts()/len(HVC_data))*100)\n",
    "((HVC_data['Churn'].value_counts()/len(HVC_data))*100).plot(kind=\"pie\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b64fab91",
   "metadata": {},
   "source": [
    "It is clearly evident from the above pie chart that there is a class imbalance with a churn of nearly 8% and Not_churn of nearly 92%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e122cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "HVC_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0daf8994",
   "metadata": {},
   "source": [
    "## Step 4:\n",
    "##  Data preparation\n",
    "## i.Deriving new variables to understand the data\n",
    "## ii.EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6145565b",
   "metadata": {},
   "source": [
    "The variable 'aon' can be converted as 'tenure_in_months' and store it in seperate dataframe for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f06078",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a new variable 'tenure_in_months'\n",
    "HVC_data_tenure_in_months = (HVC_data['aon']/30).round(0)\n",
    "\n",
    "# Since we derived a new column from 'aon', we can drop it\n",
    "#HVC_data.drop('aon',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ca6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the distribution of 'tenure_in_months'\n",
    "sns.distplot(HVC_data_tenure_in_months,bins=30)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e583a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tenure_interval = [0, 6, 12, 24, 60, 61]\n",
    "tenure_label = [ '0-6 Months', '6-12 Months', '1-2 Yrs', '2-5 Yrs', '5 Yrs and above']\n",
    "HVC_data_tenure_in_months['tenure_range'] = pd.cut(HVC_data_tenure_in_months, tenure_interval, labels=tenure_label)\n",
    "HVC_data_tenure_in_months['tenure_range'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af95e15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting a barchart for tenure ranges\n",
    "plt.figure(figsize=[12,8])\n",
    "sns.barplot(x='tenure_range',y=HVC_data.Churn, data=HVC_data_tenure_in_months)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f790a6",
   "metadata": {},
   "source": [
    "From the above barchart the following are inferences:\n",
    "\n",
    "Maximum churns observed for the 0-6 months of tenure\n",
    "The churning rate decreases gradually as the tenure increases.\n",
    "Note: On account of gradual (almost linear decrement) of churning rate with the increase of tenure, we prefer to keep the original variable as bucketing does not improve prediction of churn (and hence, we do not merge this dataframe with the original datafame)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d60acf0",
   "metadata": {},
   "source": [
    "Let's aggregate all existing the months of 6 and 7 to a single corresponding columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a95bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the columns\n",
    "sorted(list(HVC_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2951832d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a list of column names for the month 6 and 7\n",
    "\n",
    "month_6_cols = [col for col in HVC_data.columns if '_6' in col]\n",
    "month_7_cols = [col for col in HVC_data.columns if '_7' in col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b87238a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking whether the variable exists for both the months 6 and 7:\n",
    "avg_6_7_columns = []\n",
    "\n",
    "for col_1 in month_6_cols:\n",
    "    for col_2 in month_7_cols:\n",
    "        if col_1[:-2] == col_2[:-2]:\n",
    "            avg_6_7_columns.append(col_1[:-2] + \"_6_7\")\n",
    "set_avg_6_7_columns = set(avg_6_7_columns)\n",
    "avg_6_7_columns =list(set_avg_6_7_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8907390c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in avg_6_7_columns:\n",
    "    if element[-6:] == '_6_6_7':\n",
    "        avg_6_7_columns.remove(element)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a04f489",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(avg_6_7_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92016d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg_6_7_columns\n",
    "month_6_cols = []\n",
    "month_7_cols = []\n",
    "for col in avg_6_7_columns:\n",
    "    month_6_cols.append(str(col[:-4] + '_6'))\n",
    "    month_7_cols.append(str(col[:-4] + '_7'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdabb38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(avg_6_7_columns)):\n",
    "    HVC_data[avg_6_7_columns[i]] = (HVC_data[month_6_cols[i]] + HVC_data[month_7_cols[i]])/2    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71b1a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "HVC_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a6f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "HVC_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "473ac9f7",
   "metadata": {},
   "source": [
    "We drop the mobile_number column as it acts as unique ID and doest have any predictive value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f2e917",
   "metadata": {},
   "outputs": [],
   "source": [
    "HVC_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df21cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(HVC_data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141af34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets check the correlation between independent variables with the target variable- 'Churn'\n",
    "plt.figure(figsize=(6,36))\n",
    "sns.heatmap(HVC_data.corr()[['Churn']].sort_values(by='Churn', ascending=False), annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaa721d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets check the correlation amongst the independent variables, drop the highly correlated ones\n",
    "HVC_corr = HVC_data.corr()\n",
    "\n",
    "#Stacking the columns into single column for easy readability\n",
    "HVC_corr = HVC_corr.unstack()\n",
    "\n",
    "#Filtering the highly correlated columns\n",
    "HVC_corr = HVC_corr[((HVC_corr > 0.85) & (HVC_corr < 1)) | ((HVC_corr < -0.85) & (HVC_corr > -1))].sort_values(ascending=False) \n",
    "\n",
    "HVC_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfba16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing some high related columns above 86% based on the corelation and business knowledge\n",
    "high_correl_cols = ['sachet_2g_8','sachet_2g_6_7','isd_og_mou_8',\n",
    "                   'loc_ic_mou_8','loc_ic_mou_6_7','sachet_3g_8','sachet_3g_6_7',] \n",
    "HVC_data.drop(high_correl_cols, axis=1, inplace=True)\n",
    "HVC_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dd1ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "HVC_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd7e66c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in HVC_data.columns:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83f54d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "HVC_data['onnet_min']=HVC_data['onnet_mou_6']+HVC_data['onnet_mou_7']+HVC_data['onnet_mou_8']\n",
    "HVC_data['offnet_min']=HVC_data['offnet_mou_6']+HVC_data['offnet_mou_7']+HVC_data['offnet_mou_8']\n",
    "HVC_data.drop(['onnet_mou_6','onnet_mou_7','onnet_mou_8','offnet_mou_6','offnet_mou_7','offnet_mou_8'],1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f15a9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mou=[]\n",
    "for i in HVC_data.columns:\n",
    "    if 'mou' in i:\n",
    "        mou.append(i)\n",
    "mou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8205be",
   "metadata": {},
   "outputs": [],
   "source": [
    "HVC_data.drop(mou,1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a917cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "HVC_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0056068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "HVC_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e46a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "HVC_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24dce419",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (50, 50))\n",
    "sns.heatmap(HVC_data.corr())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ece1c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "HVC_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a23c241",
   "metadata": {},
   "outputs": [],
   "source": [
    "churn_rate = (sum(HVC_data[\"Churn\"])/len(HVC_data[\"Churn\"].index))*100\n",
    "churn_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6803cecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train.iloc[:,31:34]=X_train.iloc[:,31:34].astype('int64')\n",
    "#X_train.iloc[:,25:26]=X_train.iloc[:,25:26].astype('int64')\n",
    "#X_train.iloc[:,13:14]=X_train.iloc[:,13:14].astype('int64')\n",
    "#X_train.iloc[:,19:20]=X_train.iloc[:,19:20].astype('int64')\n",
    "#X_train.iloc[:,16:17]=X_train.iloc[:,16:17].astype('int64')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2104527",
   "metadata": {},
   "source": [
    "## Step-5 :\n",
    "### Data modeling and evaliation \n",
    "\n",
    "### Split data in to train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7bb989",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating X and y variables\n",
    "X = HVC_data.drop(['Churn'],axis=1)\n",
    "y = HVC_data['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b10d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7c8122",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63729ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dateset into train and test datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.7, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c07a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff7fda7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4522f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8077edf0",
   "metadata": {},
   "source": [
    "## Performing scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76e14a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the features\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train[X_train.columns] = scaler.fit_transform(X_train[X_train.columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28f5084",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check version number\n",
    "import imblearn\n",
    "print(imblearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06650c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86eb8f6",
   "metadata": {},
   "source": [
    "## handling data imbalence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1566e61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Imbalance Handling\n",
    "# we employ SMOTE method to handle the data imbalance with target variable\n",
    "import imblearn\n",
    "from imblearn.over_sampling import SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_smote,y_train_smote = smote.fit_resample(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae05728",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_smote.shape)\n",
    "print(y_train_smote.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2b73e1",
   "metadata": {},
   "source": [
    "We'll use these data sets X_train_smote and y_train_smote throughout various models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364c637b",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "### Model-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32695915",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries for Model creation\n",
    "import statsmodels.api as sm\n",
    "\n",
    "log_reg_1 = sm.GLM(y_train_smote,(sm.add_constant(X_train_smote)), family = sm.families.Binomial())\n",
    "log_reg_1.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fbf75d",
   "metadata": {},
   "source": [
    "## Logistic Regression with RFE Features\n",
    "## Model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bc4d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing RFE and LinearRegression\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LinearRegression\n",
    "log_reg_rfe = LogisticRegression()\n",
    "\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# running RFE with 25 variables as output\n",
    "rfe = RFE(log_reg_rfe, n_features_to_select=25)             \n",
    "rfe = rfe.fit(X_train_smote, y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a28938",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_columns = X_train_smote.columns[rfe.support_]\n",
    "rfe_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d7c09",
   "metadata": {},
   "outputs": [],
   "source": [
    "list(zip(X_train_smote.columns, rfe.support_, rfe.ranking_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72580e03",
   "metadata": {},
   "source": [
    "Apply manual method to evaluate the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df6096f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression model_2\n",
    "X_train_smote_2 = sm.add_constant(X_train_smote[rfe_columns])\n",
    "log_reg_2 = sm.GLM(y_train_smote,X_train_smote_2, family = sm.families.Binomial())\n",
    "res = log_reg_2.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd04771a",
   "metadata": {},
   "source": [
    "As looking at the  p-values all are in the acceptable range (i.e. p-value > 0.05). Hence,no droping of any variable. Before procede to the next step we can check the VIFs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98fae3c4",
   "metadata": {},
   "source": [
    "## VIF_2\n",
    "checking the VIF values for model_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2186d387",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_smote_2[rfe_columns].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_smote_2[rfe_columns].values, i) for i in range(X_train_smote_2[rfe_columns].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9389b163",
   "metadata": {},
   "source": [
    "based on above vif table the high vif value variable arpu_6_7 with \t93.21 value .But we can go on with  its p-value is > 0.05."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c2d2ff9",
   "metadata": {},
   "source": [
    "## Model - 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f7f4310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression model_3\n",
    "X_train_smote_3 = sm.add_constant(X_train_smote[rfe_columns])\n",
    "log_reg_3 = sm.GLM(y_train_smote,X_train_smote_3, family = sm.families.Binomial())\n",
    "res = log_reg_3.fit()\n",
    "res.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e4d4f7",
   "metadata": {},
   "source": [
    "p-values are remain within the acceptable limits (p-value < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694f73c5",
   "metadata": {},
   "source": [
    "## VIF_3\n",
    "checking the VIF values for model_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1effa0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_smote_3[rfe_columns].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_smote_3[rfe_columns].values, i) for i in range(X_train_smote_3[rfe_columns].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdcef67",
   "metadata": {},
   "source": [
    "From the above table of VIF, the variable 'overall_rech_amt_6_7' holds different VIF value, hence, we drop this variable to reduce the multi-collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd280a02",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfe_columns_3=rfe_columns.drop(\"overall_rech_amt_6_7\",1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2c8162",
   "metadata": {},
   "source": [
    "## Model - 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dacad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression model_4\n",
    "X_train_smote_4 = sm.add_constant(X_train_smote[rfe_columns_3])\n",
    "log_reg_4 = sm.GLM(y_train_smote,X_train_smote_4, family = sm.families.Binomial())\n",
    "res = log_reg_4.fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93835ac5",
   "metadata": {},
   "source": [
    "## VIF_4\n",
    "checking the VIF values for model_4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beb58e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe that will contain the names of all the feature variables and their respective VIFs\n",
    "vif = pd.DataFrame()\n",
    "vif['Features'] = X_train_smote_4[rfe_columns_3].columns\n",
    "vif['VIF'] = [variance_inflation_factor(X_train_smote_4[rfe_columns_3].values, i) for i in range(X_train_smote_4[rfe_columns_3].shape[1])]\n",
    "vif['VIF'] = round(vif['VIF'], 2)\n",
    "vif = vif.sort_values(by = \"VIF\", ascending = False)\n",
    "vif"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd2a0e1",
   "metadata": {},
   "source": [
    "All VIF values are within the acceptable limits(~5 or less)except some importent features for the future modeling. Hence, the resultant features are significant without any multi-collinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9948b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_smote_4.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8fa5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the predicted values on the train set\n",
    "y_train_pred = res.predict(X_train_smote_4)\n",
    "y_train_pred =y_train_pred.values.reshape(-1)\n",
    "y_train_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54278a71",
   "metadata": {},
   "source": [
    " Creating a dataframe with the actual churn flag and the predicted probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eb37a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_pred_final = pd.DataFrame({'Churn':y_train_smote.values, 'Churn_Prob':y_train_pred})\n",
    "y_train_pred_final['CustID'] = y_train_smote.index\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e99c0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    " #Creating new column 'predicted' with 1 if Churn_Prob > 0.5 else 0\n",
    "y_train_pred_final['predicted'] = y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > 0.5 else 0)\n",
    "\n",
    "# Let's see the head\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed208e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's take a look at the confusion matrix again \n",
    "from sklearn import metrics\n",
    "confusion = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final.predicted )\n",
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138f7560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's check the overall accuracy.\n",
    "print(metrics.accuracy_score(y_train_pred_final.Churn, y_train_pred_final.predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e5e7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Metrics beyond simply accuracy\n",
    "TP = confusion[1,1] # true positive \n",
    "TN = confusion[0,0] # true negatives\n",
    "FP = confusion[0,1] # false positives\n",
    "FN = confusion[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c867558b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sensitivity \n",
    "print(\"Sensitivity = \",TP / float(TP+FN))\n",
    "\n",
    "# Calculate specificity\n",
    "print(\"Specificity = \",TN / float(TN+FP))\n",
    "\n",
    "# Calculate False Positive Rate\n",
    "print(\"False Positive Rate = \",FP/ float(TN+FP))\n",
    "\n",
    "# Calculate Precision\n",
    "print (\"Precision = \",TP / float(TP+FP))\n",
    "\n",
    "# Caclulate Negative predictive value\n",
    "print (\"Negative Prediction Rate = \",TN / float(TN+FN))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43fc1f6",
   "metadata": {},
   "source": [
    "## Plotting the ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020ee592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_roc( actual, probs ):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve( actual, probs,\n",
    "                                              drop_intermediate = False )\n",
    "    auc_score = metrics.roc_auc_score( actual, probs )\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot( fpr, tpr, label='ROC curve (area = %0.2f)' % auc_score )\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate or [1 - True Negative Rate]')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fb5ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = metrics.roc_curve( y_train_pred_final.Churn, y_train_pred_final.Churn_Prob, drop_intermediate = False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846a047b",
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_roc(y_train_pred_final.Churn, y_train_pred_final.Churn_Prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85acd4f4",
   "metadata": {},
   "source": [
    "## Finding Optimal Cutoff Point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb3770eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's create columns with different probability cutoffs \n",
    "numbers = [float(x)/10 for x in range(10)]\n",
    "for i in numbers:\n",
    "    y_train_pred_final[i]= y_train_pred_final.Churn_Prob.map(lambda x: 1 if x > i else 0)\n",
    "y_train_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd52d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's calculate accuracy sensitivity and specificity for various probability cutoffs.\n",
    "cutoff_df = pd.DataFrame( columns = ['prob','accuracy','sensi','speci'])\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# TP = confusion[1,1] # true positive \n",
    "# TN = confusion[0,0] # true negatives\n",
    "# FP = confusion[0,1] # false positives\n",
    "# FN = confusion[1,0] # false negatives\n",
    "\n",
    "num = [0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "for i in num:\n",
    "    cm1 = metrics.confusion_matrix(y_train_pred_final.Churn, y_train_pred_final[i] )\n",
    "    total1=sum(sum(cm1))\n",
    "    accuracy = (cm1[0,0]+cm1[1,1])/total1\n",
    "    \n",
    "    speci = cm1[0,0]/(cm1[0,0]+cm1[0,1])\n",
    "    sensi = cm1[1,1]/(cm1[1,0]+cm1[1,1])\n",
    "    cutoff_df.loc[i] =[ i ,accuracy,sensi,speci]\n",
    "print(cutoff_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961117e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's plot accuracy sensitivity and specificity for various probabilities.\n",
    "cutoff_df.plot.line(x='prob', y=['accuracy','sensi','speci'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c36ef3",
   "metadata": {},
   "source": [
    "From the curve above, 0.5 is the optimum point to take it as a cutoff probability hence we will not change the optimum point"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c51b6ba",
   "metadata": {},
   "source": [
    "## Making predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00520c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the test data\n",
    "X_test[X_train.columns] = scaler.transform(X_test[X_train.columns])\n",
    "X_test_df = X_test.copy() # For future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d779871",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature selection\n",
    "X_test=X_test[rfe_columns_3]\n",
    "\n",
    "# Adding constant to the test model.\n",
    "X_test_SM = sm.add_constant(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde100a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the target variable\n",
    "y_test_pred = res.predict(X_test_SM)\n",
    "y_pred = pd.DataFrame(y_test_pred)\n",
    "y_pred=y_pred.rename(columns = {0:\"Prob\"})\n",
    "y_pred.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af60da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_df = pd.DataFrame(y_test)\n",
    "y_pred_final = pd.concat([y_test_df,y_pred],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b922583",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_final['test_churn_pred'] = y_pred_final.Prob.map(lambda x: 1 if x>0.54 else 0)\n",
    "y_pred_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33642ff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the overall accuracy of the predicted set.\n",
    "metrics.accuracy_score(y_pred_final.Churn, y_pred_final.test_churn_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451e4429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "confusion2_test = metrics.confusion_matrix(y_pred_final.Churn, y_pred_final.test_churn_pred)\n",
    "confusion2_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c44477",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Metrics beyond simply accuracy\n",
    "TP2 = confusion2_test[1,1] # true positive \n",
    "TN2 = confusion2_test[0,0] # true negatives\n",
    "FP2 = confusion2_test[0,1] # false positives\n",
    "FN2 = confusion2_test[1,0] # false negatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2375e049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sensitivity \n",
    "print(\"Sensitivity = \",TP2 / float(TP2+FN2))\n",
    "\n",
    "# Calculate specificity\n",
    "print(\"Specificity = \",TN2 / float(TN2+FP2))\n",
    "\n",
    "# Calculate False Positive Rate\n",
    "print(\"False Positive Rate = \",FP2/ float(TN2+FP2))\n",
    "\n",
    "# Calculate Precision\n",
    "print (\"Precision = \",TP2 / float(TP2+FP2))\n",
    "\n",
    "# Caclulate Negative predictive value\n",
    "print (\"Negative Prediction Rate = \",TN2 / float(TN2+FN2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c493df04",
   "metadata": {},
   "source": [
    "##  With PCA\n",
    "Model-1\n",
    "We already have the balanced train data in X_train_smote and y_train_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e1da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train_smote.shape)\n",
    "print(y_train_smote.shape)\n",
    "print(X_test_df.shape)\n",
    "print(y_test_df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e482e7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing PCA\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(random_state=48)\n",
    "\n",
    "# applying PCA on train data\n",
    "pca.fit(X_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da615dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_smote_pca = pca.fit_transform(X_train_smote)\n",
    "X_test_pca = pca.transform(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8e7ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Logistic Regression using all principal components\n",
    "\n",
    "log_reg_pca_all = LogisticRegression()\n",
    "log_reg_pca_all.fit(X_train_smote_pca, y_train_smote)\n",
    "\n",
    "# making the predictions\n",
    "y_pred = log_reg_pca_all.predict(X_test_pca)\n",
    "\n",
    "# converting the prediction into a dataframe\n",
    "y_pred_df = pd.DataFrame(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77b2c6f",
   "metadata": {},
   "source": [
    "Evaluating the performance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910aaff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "# Checking the Confusion matrix\n",
    "conf_matrix_pca_all = confusion_matrix(y_test_df,y_pred)\n",
    "\n",
    "# Checking the Accuracy of the Predicted model.\n",
    "accuracy_pca_all = accuracy_score(y_test_df,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fa5799",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conf_matrix_pca_all)\n",
    "print(accuracy_pca_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5431cc11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking the components of pca\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919e6243",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumul_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "# Making a scree plot\n",
    "fig = plt.figure(figsize=[16,10])\n",
    "plt.plot(cumul_variance)\n",
    "plt.xlabel('No of Principal Components')\n",
    "plt.ylabel('Cumulative Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5174beff",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5790a7c",
   "metadata": {},
   "source": [
    "From the above cumulative variance numbers, it is observed that the 17 components explains about 95% of variance in dependent variable and 10 principal components explains about 90% of variance.\n",
    "*We build the logistic regression model using these first 17 and 10 principal components seperately and evaluate the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714f4b61",
   "metadata": {},
   "source": [
    "## Model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f760d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_17 = PCA(n_components=17)\n",
    "train_pca_17 = pca_17.fit_transform(X_train_smote)\n",
    "test_pca_17 = pca_17.transform(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed99bbce",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_pca_17 = LogisticRegression()\n",
    "log_reg_pca_17.fit(train_pca_17, y_train_smote)\n",
    "\n",
    "# making the predictions\n",
    "y_pred_17 = log_reg_pca_17.predict(test_pca_17)\n",
    "\n",
    "# converting the prediction into a dataframe\n",
    "y_pred_17_df = pd.DataFrame(y_pred_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed20db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Confusion matrix\n",
    "conf_matrix_pca_17 = confusion_matrix(y_test,y_pred_17)\n",
    "accuracy_pca_17 = accuracy_score(y_test,y_pred_17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5984d123",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conf_matrix_pca_17)\n",
    "print(accuracy_pca_17)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edd32e8",
   "metadata": {},
   "source": [
    "The accuracy obtained by 17 principal components is significant and reduced only marginally compared to that of model from all the components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a777ec",
   "metadata": {},
   "source": [
    "Model-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b729bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_10 = PCA(n_components=10)\n",
    "\n",
    "train_pca_10 = pca_10.fit_transform(X_train_smote)\n",
    "test_pca_10 = pca_10.transform(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4506eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg_pca_10 = LogisticRegression()\n",
    "log_reg_pca_10.fit(train_pca_10, y_train_smote)\n",
    "\n",
    "# making the predictions\n",
    "y_pred_10 = log_reg_pca_10.predict(test_pca_10)\n",
    "\n",
    "# converting the prediction into a dataframe\n",
    "y_pred_10_df = pd.DataFrame(y_pred_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49af26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Confusion matrix\n",
    "conf_matrix_pca_10 = confusion_matrix(y_test,y_pred_10)\n",
    "accuracy_pca_10 = accuracy_score(y_test,y_pred_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c262af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conf_matrix_pca_10)\n",
    "print(accuracy_pca_10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ab3de0",
   "metadata": {},
   "source": [
    "The accuracy measure though acceptable has dropped significantly from the base model. Hence, we consider the model with 17 principal components."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a216a",
   "metadata": {},
   "source": [
    "## Random Forest\n",
    "We already have the balanced train in X_train_smote and y_train_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ef6cb22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier()\n",
    "rfc.fit(X_train_smote,y_train_smote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ca7860",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfc = rfc.predict(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd7a5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Confusion matrix\n",
    "conf_matrix_rfc = confusion_matrix(y_test,y_pred_rfc)\n",
    "accuracy_rfc = accuracy_score(y_test,y_pred_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e66bf07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Confusion matrix\n",
    "conf_matrix_rfc = confusion_matrix(y_test,y_pred_rfc)\n",
    "accuracy_rfc = accuracy_score(y_test,y_pred_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fc147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(conf_matrix_rfc)\n",
    "print(accuracy_rfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3d8aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rfc_best = rfc.predict(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abb9961",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the Confusion matrix\n",
    "conf_matrix_rfc_best = confusion_matrix(y_test,y_pred_rfc_best)\n",
    "accuracy_rfc_best = accuracy_score(y_test,y_pred_rfc_best)\n",
    "\n",
    "print(conf_matrix_rfc_best)\n",
    "print(accuracy_rfc_best)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f4b5252",
   "metadata": {},
   "source": [
    "Note that the best parameters procuded the accuracy of 91% which is not significantly deterred than the accuracy of original random forest, which is pegged around 92%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88d7ff8",
   "metadata": {},
   "source": [
    "Conclusion\n",
    "The best model to predict the churn is observed to be Random Forest based on the accuracy as performance measure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773a883b",
   "metadata": {},
   "source": [
    "The incoming calls (with local same operator mobile/other operator mobile/fixed lines, STD or Special) plays a vital role in understanding the possibility of churn. Hence, the operator should focus on incoming calls data and has to provide some kind of special offers to the customers whose incoming calls turning lower."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8aee4f8",
   "metadata": {},
   "source": [
    "Notes:\n",
    "After cleaning the data, we broadly employed three models as mentioned below including some variations within these models in order to arrive at the best model in each of the cases.\n",
    "\n",
    "Logistic Regression\n",
    "Logistic Regression with RFE\n",
    "Logistic regression with PCA\n",
    "Random Forest\n",
    "For each of these models, the summary of performance measures are as follows:\n",
    "\n",
    "Logistic Regression\n",
    "\n",
    "Accuracy: 78%\n",
    "\n",
    "Logistic regression with PCA\n",
    "\n",
    "Accuracy (all principal components includes): ~78%\n",
    "Accuracy (all principal components includes): ~74%\n",
    "Accuracy (all principal components includes): ~60%\n",
    "\n",
    "Random Forest\n",
    "\n",
    "Accuracy (without hyperparameter tuning): ~92%\n",
    "Accuracy (with hyperparameter tuning): ~91%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa768cb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
